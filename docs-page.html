<!DOCTYPE html>
<html lang="en">
<head>
    <title>Quest Quill</title>

    <!-- Meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="description" content="Bootstrap 4 Template For Software Startups">
    <meta name="author" content="Xiaoying Riley at 3rd Wave Media">
    <link rel="shortcut icon" href="favicon.ico">

    <!-- Google Font -->
    <link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,600,700&display=swap" rel="stylesheet">

    <!-- FontAwesome JS-->
    <script defer src="assets/fontawesome/js/all.min.js"></script>

    <!-- Plugins CSS -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.2/styles/atom-one-dark.min.css">
    <link rel="stylesheet" href="assets/plugins/simplelightbox/simple-lightbox.min.css">

    <!-- Theme CSS -->
    <link id="theme-style" rel="stylesheet" href="assets/css/theme.css">

</head>

<body class="docs-page">
    <header class="header fixed-top">
        <div class="branding docs-branding">
            <div class="container-fluid position-relative py-2">
                <div class="docs-logo-wrapper">
					<button id="docs-sidebar-toggler" class="docs-sidebar-toggler docs-sidebar-visible me-2 d-xl-none" type="button">
	                    <span></span>
	                    <span></span>
	                    <span></span>
	                </button>
	                <div class="site-logo"><a class="navbar-brand" href="index.html"><img class="logo-icon me-2" src="assets/images/coderdocs-logo.svg" alt="logo"><span class="logo-text">Quest Quil<span class="text-alt"></span></span></a></div>
                </div><!--//docs-logo-wrapper-->
	            <div class="docs-top-utilities d-flex justify-content-end align-items-center">
	                <div class="top-search-box d-none d-lg-flex">
		                <form class="search-form">
				            <input type="text" placeholder="Search the docs..." name="search" class="form-control search-input">
				            <button type="submit" class="btn search-btn" value="Search"><i class="fas fa-search"></i></button>
				        </form>
	                </div>

					<ul class="social-list list-inline mx-md-3 mx-lg-5 mb-0 d-none d-lg-flex">
						<li class="list-inline-item"><a href="https://github.com/kittog/quest-quill"><i class="fab fa-github fa-fw"></i></a></li>
		            </ul><!--//social-list-->
	            </div><!--//docs-top-utilities-->
            </div><!--//container-->
        </div><!--//branding-->
    </header><!--//header-->


    <div class="docs-wrapper">
	    <div id="docs-sidebar" class="docs-sidebar">
		    <div class="top-search-box d-lg-none p-3">
                <form class="search-form">
		            <input type="text" placeholder="Search the docs..." name="search" class="form-control search-input">
		            <button type="submit" class="btn search-btn" value="Search"><i class="fas fa-search"></i></button>
		        </form>
            </div>
		    <nav id="docs-nav" class="docs-nav navbar">
			    <ul class="section-items list-unstyled nav flex-column pb-3">
				    <li class="nav-item section-title"><a class="nav-link scrollto active" href="#section-1"><span class="theme-icon-holder me-2"><i class="fas fa-map-signs"></i></span>Introduction</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-2"><span class="theme-icon-holder me-2"><i class="fas fa-arrow-down"></i></span>Installation</a></li>

				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-3"><span class="theme-icon-holder me-2"><i class="fas fa-box"></i></span>Modèle GPT2 FineTuning</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-1">Données utilisées</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-2">Prétraitement</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-3">Fine Tunning GPT2</a></li>
            <li class="nav-item"><a class="nav-link scrollto" href="#item-3-4">Intégration avec FastAPI</a></li>

				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-5"><span class="theme-icon-holder me-2"><i class="fas fa-tools"></i></span>Résultats</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-4-1">Visualisation de certains output et évaluation inter-annotateurs</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-4-2">Évaluation de Quest Quill</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-4-2-1">Évaluation perceptive</a></li>
            <li class="nav-item"><a class="nav-link scrollto" href="#item-4-2-2">Évaluation avec la métrique GRUEN</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-5"><span class="theme-icon-holder me-2"><i class="fas fa-laptop-code"></i></span>Essai avec BERT</a></li>
				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-6"><span class="theme-icon-holder me-2"><i class="fas fa-tablet-alt"></i></span>Contact</a></li>
            <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-7"><span class="theme-icon-holder me-2"><i class="fas fa-tablet-alt"></i></span>Bibliographie</a></li>


			    </ul>

		    </nav><!--//docs-nav-->
	    </div><!--//docs-sidebar-->
	    <div class="docs-content">
		    <div class="container">
			    <article class="docs-article" id="section-1">
				    <header class="docs-header">
					    <h1 class="docs-heading">Quest Quill, qu'est ce que c'est ? <span class="docs-time">Last updated: 2024-02-28</span></h1>
					    <section class="docs-intro">
						    <p>Quest Quill est un générateur de quêtes automatiques auquel l'utilisateur spécifie la carte où il souhaite combattre, le monstre auquel il souhaite se confronter, ainsi que la difficulté globale de la quête désirée.</p>
							<p>Suite aux indications données par l'utilisateur, Quest Quill va générer une quête avec un contexte et un objectif à accomplir.</p>
							<p style="text-align: justify;"><strong>Et pourquoi ?</strong></p>
							<p>Le monde étant en grand danger suite aux multiples attaques de monstres, il nous a semblé urgent de nous entraîner à les combattre à l'aide d'un générateur de quêtes automatiques. Les objectifs de cet entraînement sont,
                dans un premier temps, générer des quêtes avec des conditions telles que : <i>map</i>, <i>difficulté</i>, <i>monstre</i>. Dans un second temps, les quêtes devront être cohérente et présenter quelques touches d'humour.
                (À quoi bon autant s'entraîner si on ne peut pas un petit peu s'amuser...)</p>
                <p style="text-align: justify;"><strong>Notre plan d'action</strong></p>
                <p>Pour sauver le monde, il faut être <i>organisé</i>. Ainsi, le <b>pré-traitement</b> fut d'abord réalisé en groupe : il fallait trouver l'input idéal pour notre modèle de génération. L'implémentation du code pour l'<b>entraînement de notre modèle GPT-2</b> fut réalisé par Natacha : c'est également Natacha
                qui gérait l'entraînement du modèle (il faut une sacrée machine pour faire ça, et <i>heureusement</i> pour nous, Natacha est équipée). La génération a été reéalisée sur la même machine mais en appel Zoom afin qu'on puisse toutes voir les problèmes de la génération. Un des problème a été la mauvaise génération de la section objective de la quête. Pour régler ça Léna a approfonfis le pré-traitement en retirant quelques monstres et map. Le problème est réglé =D </p>
                <p> Nous avons continué de discuter
              au fil des essais de génération et entraînements afin de trouver comment peaufiner le modèle et obtenir les meilleures quêtes possibles. De son côté, dans l'optique d'obtenir des résultats comparables à ceux de GPT-2, Qinliang a réalisé des <b>essais avec le modèle BERT</b>,
            qu'elle a pu finetuné. L'<b>API</b> de Quest Quill fut conçue par Natacha. Finalement, après avoir évalué nos quêtes générées entre nous, Léna a mené une <b>étude perceptive</b> à l'aide de 26 participants,
          ainsi qu'une métrique (GRUEN) pour compléter <b>l'évaluation du modèle. Le site quant à lui est une template que Qi et Léna ont revu afin de l'adapter à notre projet</b>.</p>



						</section><!--//docs-intro-->


				    </header>



			    </article>

			    <article class="docs-article" id="section-2">
				    <header class="docs-header">
					    <h1 class="docs-heading">Installation</h1>
						<section class="docs-intro">

							<head>
						</section><!--//docs-intro-->

				    </header>
				     <section class="docs-section" id="item-2-1">
						<h2 class="section-heading"></h2>
						<p><strong>Pour installer Quest Quill, suivez ces étapes :</strong></p>

						<ol>
							<li>Clonez le dépôt : <code>git clone git@github.com:kittog/quest-quill.git</code></li>
							<li>Accédez au répertoire du projet : <code>cd quest-quill</code></li>
							<li>Installez les dépendances : <code>pip install -r requirements.txt</code></li>
						</ol>
						<p><strong>Pour utiliser Quest Quill, suivez ces étapes :</strong></p>

						<ol>
							<li>Téléchargez le modèle disponible sur WeTransfer.</li>
							<li>Accédez au dossier de l'API : <code>cd/api</code></li>
							<li>Décompressez le fichier .rar téléchargé sur WeTransfer dans le dossier API.</li>
							<li>Exécutez l'application : <code>python -m uvicorn main:app --reload</code></li>
							<li>Suivez les invites pour spécifier les paramètres de la quête tels que la difficulté, le cadre et le type de quête.</li>
							<li>Votre quête est générée ! Nous espérons qu'elle vous plaît !</li>
						</ol>
						<p><strong>Comment générer une quête une fois sur l'interface ?</strong> </p>
						<p>Afin de générer la quête, il faut mentionner dans l'espace dédié:</p>

								<ul>
									<li>La <b>carte</b> où se déroulera la quête pour pouvoir s'acclimater à divers environnements de combat.</li>
									<li>La <b>difficulté</b> de la quête à choisir entre easy, medium, hard</li>
									<li>Le ou les <b>monstres</b> à chasser.</li>
								</ul>				    <style>
									table {
										width: 100%;
										border-collapse: collapse;
									}
									th, td {
										border: 1px solid black;
										padding: 8px;
										text-align: left;
									}
									th {
										background-color: #f2f2f2;
									}
								</style>
							</head>
							<body>

							<h2>Bestiaire A COMPLETER</h2>

							<table>
								<tr>
									<td>Jagras</td>
									<td>Baggi</td>
									<td>Arzuros</td>
									<td>Great Baggi</td>
								</tr>
								<tr>
									<td>Lagombi</td>
									<td>Kulu-Ya-Ku</td>
									<td>Tetranadon</td>
									<td>Aknosom</td>
								</tr>
								<tr>
									<td>Great Izuchi</td>
									<td>Royal Ludroth</td>
									<td>Barroth</td>
									<td>Khezu</td>
								</tr>
								<tr>
									<td>Bishaten</td>
									<td>Somnacanth</td>
									<td>Pukei-Pukei</td>
									<td>Jyuratodus</td>
								</tr>
								<tr>
									<td>Basarios</td>
									<td>Rathian</td>
									<td>Tobi-Kadachi</td>
									<td>Volvidon</td>
								</tr>
								<tr>
									<td>Mizutsune</td>
									<td>Anjanath</td>
									<td>Rathalos</td>
									<td>Almudron</td>
								</tr>
								<tr>
									<td>Goss Harag</td>
									<td>Nargacuga</td>
									<td>Zinogre</td>
									<td>Magnamalo</td>
								</tr>
								<tr>
									<td>Tigrex</td>
									<td>Diablos</td>
									<td>Rajang</td>
									<td>Kushala Daora</td>
								</tr>
								<tr>
									<td>Teostra</td>
									<td>Chameleos</td>
									<td>Wind Serpent Ibushi</td>
									<td>Thunder Serpent Narwa</td>
								</tr>
								<tr>
									<td>Narwa the Allmother</td>
									<td>Crimson Glow Valstrax</td>
									<td>Apex Arzuros</td>
									<td>Apex Rathian</td>
								</tr>
								<tr>
									<td>Apex Mizutsune</td>
									<td>Apex Rathalos</td>
									<td>Apex Diablos</td>
									<td>Apex Zinogre</td>
								</tr>
							</table>

							</body>

							<h2>Map</h2>
							<table>
								<tr>
									<td>Shrine Ruins</td>
									<td>Frost Islands</td>
									<td>Sandy Plains</td>
									<td>Flooded Forest</td>
								</tr>
								<tr>
									<td>Lava Caverns</td>
									<td>Shrine Ruins</td>
									<td>Arena</td>
									<td>Elder's Recess</td>
								</tr>
								<tr>
									<td>Rotten Vale</td>
									<td>Hoarfrost Reach</td>
									<td>The Guiding Lands</td>
									<td>Coral Highlands</td>
								</tr>
								<tr>
									<td>Ancient Forest</td>
									<td>Wildspire Waste</td>
									<td>Jungle</td>
									<td>Seliana Supply Cache</td>
								</tr>
							</table>					</section><!--//section-->

			    </article><!--//docs-article-->

			    <article class="docs-article" id="section-3">
				    <header class="docs-header">
					    <h1 class="docs-heading">Modèle GPT2 FineTuning</h1>
					    <section class="docs-intro">
				</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-3-1">
						<h2 class="section-heading">Données utilisées</h2>
						<p style="text-align: justify;"> Les données qui ont été utilisées dans ce projet viennent d'un dépôt GitHub libre sous licence MIT : <a href="https://github.com/CrimsonNynja/monster-hunter-DB/tree/master">monster-hunter-DB</a></p>
						<p style="text-align: justify;">Ces données sont des <b>quêtes</b> qui proviennent du jeu vidéo <em>Monster Hunter</em> : un jeu de combat entre chasseurs et monstres (hyper classes et super stylés). La particularité des quêtes de Monster Hunter,
             est qu'elles sont une structure simple : un <b>monstre à tuer</b>, un <b>contexte</b>, un <b>objectif</b>, une <b>carte</b> et une <b>difficulté</b>. Ce qui est parfait pour notre projet.
             En plus de ça, ce sont des quêtes qui ont un <em>fond d'humour</em> ce qui nous permet de rire malgré le destin du monde qui s'avère bien noir...</p>
					</p>					</section><!--//section-->

					<section class="docs-section" id="item-3-2">
						<h2 class="section-heading">Prétraitement</h2>

					<p>
							<ul style="text-align: justify;">
								<li>Les données du dépôt GitHub sont sous un format JSON avec plusieurs types de balises : <code>target</code>, <code>objective</code>, <code>map</code>, <code>difficulty</code>.</li>
								<li>Le corpus d'entraînement est constitué d'un total de <b>848 quêtes</b>.</li>
								<li>À partir de ce corpus, extraction du contenu des balises qui nous intéressent : <code>difficulty</code>, <code>objective</code>, <code>target</code>, <code>description</code>, à l'aide du script <code>scripts/pretraitement.ipynb</code>.</li>
								<li>Pour simplifier la visibilité, le rendu de l'API, ainsi que le code, on met le corpus sous format <code>.txt</code> en formattant les quêtes. <b>Exemple : </b> <i>Map : Desert | Difficulty : easy | Objective : Kill a Rathian | Description blabla....</i></li>
                <li>Après quelques essais de génération de quêtes, nous nous sommes rendues compte qu'il y avait <em>trop de monstres</em> à apprendre par rapport au <em>nombre de quêtes</em> dont nous disposions. Nous rencontrions donc
                des soucis dans la génération des objectifs et des descriptions des quêtes.</li>
                <li>Simplification du corpus en retirant <b>12 monstres</b> ainsi que <b>7 maps</b> du corpus. Pour certains monstres, leurs noms ont été synthéthisés.<b> Exemple :</b> <i>Apex Nargacuga → Nargacuga.</i>
                <li>Pour la simplification, on procède avec Notedpad, et on remplace les noms des monstres/maps <b>peu fréquents</b>
              (présents dans moins de 5 quêtes) par des monstres/maps <b>plus fréquents</b>.</li>
			  <li>Les balises difficultées ont également étaient somplifié. Dans le corpus c'est la notation du jeu tel que : MR1 , HR6. Nous avons conscience que le modèle doit pouvoir être utilisé par plusieurs personnes connaissnat pas ce jeu, nous avons donc converti ces niveaux en des termes plus générique tel que : easy , medium, hard</li>
							</ul>

					</p>					</section><!--//section-->

					<section class="docs-section" id="item-3-3">
						<h2 class="section-heading">Fine Tuning GPT2</h2>
						<ul style="text-align: justify;">
							<li>L'entraînement a été réalisé avec le script <code>train.py</code></li>
							<li>Le modèle utilisé est <b>GPT2 medium</b> pour éviter les longs délais d'entraînement qu'impliquerait la version <b>large</b>.</li>
							<li>Notre corpus au format <code>.txt</code> est passé en <b>input</b>.</li>
              <br/>
              <b>Choix des paramètres pour l'entraînement :</b>
              <li>Apprentissage basé sur des blocs textuels de <em>taille 75</em>, permettant de couvrir intégralement une quête. (ce choix se justifie par la longueur moyenne d'une quête qui est de 75.)</li>
							<li>L'entraînement se déroule sur <b>10 epochs</b> avec un <b>batch size</b> de <b>10</b>.</li>
              <br/>
              <b>Choix des paramètres pour la génération :</b>
              <li>La génération des quêtes se fait à l'aide du script <code>generate.py</code></li>
							<li>Limitation à <b>75 max de n-grammes générés</b> pour la génération de texte.</li>
							<li>La température contrôle le niveau de créativité. Le choix de valeurs plus élevées génère des sorties plus variées mais moins cohérentes.</li>
							<li><b>Top-K Sampling</b> limite les choix de tokens aux <em>K tokens les plus probables</em>, favorisant la <em>cohérence</em> tout en permettant de la <em>variété</em>.</li>
							<li><b>Top-P Sampling</b> (<i>Nucleus Sampling</i>) limite la <em>distribution cumulative des probabilités</em>, permettant une génération plus <em>dynamique</em> parmi les tokens les plus probables.</li>
							<li><b>No Repeat N-Grams</b> empêche la <em>répétition immédiate</em> de séquences de mots pour éviter les <em>motifs répétitifs indésirables</em>.</li>
							<li>Les paramètres de génération peuvent être ajustés pour obtenir les résultats souhaités.</li>
							<li>Le prompt spécifié en <em>back end</em> est :  <code>Map : {map} | Dificulty : {level} | Target : {monstres}</code> </li>
							<li>Ce prompt permet d'inciter le modèle à générer la suite de la quête → <code>Objectif</code> , <code>Description</code>.</li>
							<li> Nous avons choisi les hyper paramètres pour la génération de <em>manière empirique</em> : au début nous générions des quêtes avec une <b>max length de 175</b>, ce qui était beaucoup trop. Le générateur était alors très incohérent dans les descriptions.</li>
							<li><b>top k</b> et <b>top p</b> ont été utilisés pour que les quêtes générées soient, pour chaque génération utilisant les mêmes prompts, différentes. Au départ, nous obtenions systématiquement la même quête pour certains mots clefs de départ, ce qui n'était pas du tout amusant.</li>

						</ul>

					</p>					</section><!--//section-->
			    <!--//docs-article-->

          <section class="docs-section" id="item-3-4">
						<h2 class="section-heading">Intégration avec FastAPI</h2>
						<h3>Aspect API</h3>
						<center><img src="image/interf.png"></center>
						<center><figcaption><b>Image 1 — Aperçu de l'API au démarrage.</b></figcaption></center>
						<center><img src="image/interf2.png"></center>
						<center><figcaption><b>Image 2 — Aperçu de l'API lors de la génération.</b></figcaption></center>
						<ul style="text-align: justify;">
								<li>L'API de Quest Quill fut développée à l'aide du module Python <code>FastAPI</code>.</li>
								<li>L'interface Web développée propose à l'utilisateur d'indiquer les trois critères nécessaire pour la génération de la quête souhaitée. (voir ci-dessous)</li>

                <center><img src="image/exemple.png"></center>
                <center><figcaption><b>Image 3 — Aperçu des champs à compléter pour avoir notre quête.</b></figcaption></center>
										</ul>						</section><!--//docs-intro-->

				    </header>

			    </article><!--//docs-article-->

			    <article class="docs-article" id="section-4">
				    <header class="docs-header">
					    <h1 class="docs-heading">Résultats</h1>
					    <section class="docs-intro">
							<ul style="text-align: justify;">
								<li>Une fois que l'utilisateur a rentré ses critères (<code>map</code>, <code>difficulty</code>, et <code>target</code>), Quest-quill génère l'objectif et la description
                correspondants.</li>
								<li>Ci-dessous, un aperçu de l'interface de Quest Quill après la génération d'une quête.</li>
                <center><img src="image/exemple2.png"></center>
                <center><figcaption><b>Image 4 — Exemple de quête générée avec Quest Quill.</b></figcaption></center>
										</ul>						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-4-1">
						<h2 class="section-heading">
							Visualisation de certains outputs et évaluation inter-annotateurs</h2>
						<ul style="text-align: justify;">
							<li>À notre surprise le modèle génère des quêtes avec <b>peu de fautes de grammaire</b> ou de <b>syntaxe</b>. De plus, les quêtes générées parviennent à reproduire des <b>traits d'humour</b> déjà présents dans les quêtes de référence.</li>
							<li>Nous notons cependant <b>plusieurs erreurs</b> : quelques fois, lorsque nous demandions une quêtes dans la map <b>"desert"</b>, le modèle générait une quête spécifique à la map <b>"arena"</b> (arène), en spécifiant dans la description que l'action se déroulait dans l'arène. Dans
              le questionnaire que nous avons établis pour l'étude perceptive, l'une des quêtes présentées montrait que la map choisie était la jungle, mais la description mentionnait une arène.</li>
							<li>Lorsque <b>plusieurs monstres</b> étaient listés comme <i>cible</i> (target), le modèle a tendance à <b>oublier certains monstres</b> dans la <i>description</i>, voir même à ne pas mentionner le moindre monstre. Cependant, il arrive à prendre
              en compte <b>le nombre de monstres</b> dans l'<i>objectif</i> généré (exemple : <i>Hunt all monsters.</i>)</li>
							<li>Pour les adeptes du jeu, les <b>incohérences produites</b> par notre modèle se repèrent facilement. La génération étant libre (l'utilisateur peut remplir les champs map, difficulty, target comme bon lui semble), nous pouvons créer ces "incohérences". L'intérêt ici est
              de voir si le modèle est capable de générer des combinaison qu'il n'aurait pas rencontrées lors de l'entraînement.</li>
							<li>Par exemple, demander <b>un Diablos en difficulté facile</b> n'est pas courant dans le jeu vidéo : le Diablos est un monstre compliqué à abattre. Une quête facile du jeu serait par exemple de tuer 7 petits monstres. Ainsi, si on demande au modèle de générer
                une quête facile avec pour cible un Diablos, le modèle va donner à l'utilisateur pour mission d'aller <b>chasser 7 Diablos</b>, une tâche loin d'être facile ! Abattre 7 Diablos, c'est dur, voir extrême ! BON COURAGE !</li>

									</ul>

						</p>					</section><!--//section-->

					<section class="docs-section" id="item-4-2">
						<h2 class="section-heading">Évaluation de Quest Quill</h2>
						<ul style="text-align: justify;">

							Il n'y a actuellement <b>pas de consensus</b> quant à <b>l'évaluation des modèles génératifs</b> [Celikyilmaz et. al (2021), C. van der Lee et. al (2020)]. En nous inspirant des pratiques reccommendées et de nos recherches personnelles,
              nous avons réalisé une <b>étude perceptive</b> pour avoir une évaluation humaine, et fait appel à la <b>métrique GRUEN</b> [Zhu et Bhat (2020)] pour évaluer automatiquement les descriptions des quêtes générées. Ces deux évaluations
              viennent compléter nos premières observations, présentées précédement.
              <br />
              <br />
              <section class="docs-section" id="item-4-2-1">
              <h3 class="section-heading">Évaluation perceptive</h3>
							<li>Pour l'étude perceptive, nous avons créé un <b>formulaire</b> (un en anglais, un en français) présentant aux participants <b>10 quêtes</b>, <em>5 desquelles ont été générées</em> par notre modèle. Les participants ne savent pas quelles quêtes sont générées, mais connaissent la distribution établie pour ce questionnaire. Pour chaque quête, les participants doivent répondre à la même série de question.  Nous interrogeons également les participants sur leur familiarité avec le jeu Monster Hunter.</li>
							<li>L'étude perceptive nous permet d'évaluer le <b>langage</b> et la <b>thématique</b> de chaque quête. Les critères que nous voulions évaluer humainement étaient les suivants : la <b>grammaire</b>, la <b>syntaxe</b>, la <b>cohérence</b>, le <b>langage</b> utilisé pour la quête, et la <b>quantité d'information</b> présente dans la quête. Pour chaque quête, les participants ont également un <em>champ de texte libre</em> dans lequel ils peuvent rajouter des <b>observations supplémentaires</b>. </li>
							<li>Les différents critères sont évalués avec des échelles de score allant de 1 à 5 (<i>5 point likert scale</i>).</li>
              <br/>
              <p style="text-align: justify;"><strong>Participants</strong></p>
              <li>Les formulaires furent par la suite partagés sur les réseaux sociaux, à nos ami.e.s, et d'autres joueurs de <em>Monster Hunter</em>.
                (Nous remercions par ailleurs ceux.celles qui ont partagé notre questionnaire à leurs contacts !) Tous les participants n'étaient pas nécessairement familiers
                avec le jeu, ou les JDR, mais cette variété apporte de la diversité dans les réponses obtenues. Nous avons notamment remarqué que les participants
                les moins familiers avec cet univers notaient principalement le <b>langage des quêtes</b>, tandis que les participants ayant joué à <i>Monster Hunter</i> ont été largement
                <b>plus critiques</b> quant à l'<b>association des divers critères</b>. Ces derniers faisaient d'ailleurs davantage usage des champs libres de texte !</li>

                <li>Finalement, notre étude perceptive rassemble un total de <b>26 participants</b>. Dans la littérature, il faudrait au moins une cinquantaine de participants, mais une
                  centaine reste l'idéal, si l'on souhaite avoir des résultats significatifs. Cependant, avec 26 participants, nous pouvons déjà rapporter des résultats intéressants !</li>
                <br/>
                <p style="text-align: justify;"><strong>Résultats</strong></p>
                <li>Sur nos 26 participants, <b>11 participants</b> ont répondu avoir <em>déjà joué</em> à l'un des jeux de la franchise <i>Monster Hunter</i>. Très souvent, ce sont ces participants
                  qui ont fait usage des champs libres de texte mis à disposition dans le questionnaire.</li>
                <li>Si on considère nos participants en deux groupes (ceux qui ont joué à Monster Hunter, et les autres), on remarque que si les deux groupes
                se rejoignent quant à l'évaluation du <b>langage</b> des quêtes, ils montrent leur désaccord quand il s'agit de noter la <b>cohérence</b> ainsi que la <b>quantité d'information</b> présente dans la quête.
              Étonnamment on remarque cette tendance pour les deux types de quêtes (générées ou non).</li>

              <center><img src="perceptive_study/coherence_quest2.png"></center>
  						<center><figcaption><b>Histogramme du score de cohérence pour la quête 2 (générée)</b></figcaption></center>

              <li>Pour la quête 2, nous pouvons voir que les joueurs de <i>Monster Hunter</i> montrent plus de <b>variabilité</b> dans leur réponse que les participants n'ayant pas joué.</li>

              <center><img src="perceptive_study/coherence_quest9.png"></center>
  						<center><figcaption><b>Histogramme du score de cohérence pour la quête 9 (non-générée)</b></figcaption></center>

              <li>Pour la quête 9, les deux groupes montrent davantage de <b>séparation</b> ; nous notons cependant qu'il est étonnant que les joueurs de <i>Monster Hunter</i> ne trouvent pas une quête
              directement issue de notre corpus cohérente !</li>

              <center><img src="perceptive_study/coherence_quest10.png"></center>
  						<center><figcaption><b>Histogramme du score de cohérence pour la quête 10 (générée)</b></figcaption></center>

              <li>La <i>bonne nouvelle</i>, c'est que certaines quêtes générées paraissent plutôt cohérentes pour l'ensemble de nos participants ! C'est le cas de la <b>quête n°10</b>, qui était générée,
                et ce, malgré une incohérence sur la map (la map choisie était "Jungle", mais la description indiquait "Arena").
              </li>

              <br/>
              <li>Finalement, les divers commentaires laissés apportent beaucoup d'informations quant à la perception de nos quêtes : ceux-ci relèvent donc par exemple
              les incohérences produites par notre modèle (absolument personne n'a apprécié l'idée d'aller chasser 7 Diablos en difficulté medium, à notre grande surprise !). Certaines fois, la quête générée
            ne mentionnait en description qu'un seul monstre, au lieu des deux indiqués en target, ce que les participants remarquent facilement. </li>
            <li>Nous présentons ci-dessous quelques-uns des commentaires que nous avons reçu, avec les points relevés par nos participants.</li>
          <br/>
            <table>
            <caption>
              <center><strong><b>Table 1 — Différents commentaires de nos participants</b></strong></center>
            </caption>
            <thead>
              <tr>
                <th scope="col">Critères</th>
                <th scope="col">Remarques</th>
                <th scope="col">Commentaires</th>
              </tr>
            </thead>
            <tbody>
              <tr>
              <td><strong>Niveau de difficulté</strong></td>
              <td>Incohérence avec le ou les monstres donnés, et/ou l'objectif, créant de l'incompréhension.</td>
              <td><cite>"I don't believe hunting 4 monsters including a lunastra and a ruiner nergigante is considered a "medium difficulty" but yeah, i'd at least grade this very difficult."</cite>,
              <cite>"Lavasioth is not as dangerous as elder dragons, and three elder dragons make a hard quest, not a medium quest."</cite>,
            <cite>"Deviljho and "easy" in the same quest ?"</cite></td></tr>
            <tr>
            <td><strong>Quantité d'information</strong></td>
            <td>Manque de contexte, d'information, incohérences. </td>
            <td><cite>"Tips for fight can be easly add in there like some funny way to say tail head and wings are his weak spots"</cite>,
            <cite>"What equipment are we talking about?"</cite>,
            <cite>"I don’t understand if the equipment is required to accomplish the quest or not"</cite></td></tr>
            <tr>
            <td><strong>Langage utilisé</strong></td>
            <td>Caractère conversationnel de la description (des fois perçu comme peu conventionnel)</td>
            <td><cite>"The monster don't really interact with the npc, and why would you hunt it, why do he want to know how it work ?""</cite>
            <cite>"Flavour text is a bit mid."</cite>
            <cite>"Interesting that the quest descriptions are more conversational than directly giving instructions."</cite></td>
          </tr>
          <tr>
            <td><strong>Type de quête</strong></td>
            <td>Quête explicitée comme étant "finale", mais ce caractère ne se montre pas dans la description.</td>
            <td><cite>"Considering that it's supposed to be a "last quest", I'd rather increase the difficulty level"</cite>
            <cite>"A final quest should have more enigmatic aspect you shouldn't be aware of what you are going to face and what's happening to you, more majestic word cause you are gona become a legend so we need to feel it"</cite>
          <cite>"Also, I find this very weird as a last quest. It'd usually be a standalone monster, like a shagaru magala or a xeno'jiiva or two ancient dragons or whatever so this seems like a pretty weird set of monsters to me."</cite></td>
        </tr>
            </thead>
          </tbody>
        </table>

        <br/>
        <p>Finalement, nous trouvons que les résultats obtenus à travers notre étude perceptive assez intéressants : ceux-ci nous donnent pas mal d'indications
        pour améliorer notre modèle, mais montrent que pour l'instant, Quest Quill se montre plutôt performant !</p>


												</section><!--//section-->

					<section class="docs-section" id="item-4-2-2">
							<h3 class="section-heading">Évaluation avec la métrique GRUEN</h3>
              La métrique <b>GRUEN</b> [Zhu et Bhat(2020)] est un score allant de 0 à 1. Plus le score est élevé, meilleur le text généré est (en ne prenant en compte que le langage).
              L'intérêt de la métrique GRUEN est qu'elle évalue l'output du modèle seulement (contrairement à d'autres métriques, développées davantage pour la traduction automatique) selon quatre critères : la grammaticalité, la non-redondance, le "focus" (sujet), la structure et la cohérence. </li>
              <br />
              <br />
              <table>
              <caption>
                <center><strong><b>Table 2 — Scores GRUEN pour 5 de nos quêtes générées</b></strong></center>
              </caption>
              <thead>
                <tr>
                  <th scope="col">QUEST_ID</th>
                  <th scope="col">QUEST</th>
                  <th scope="col">GRUEN</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <th scope="row">1</th>
                  <td>"Yo Ace Ready for some awesome Arena action Prove your mettle using the equipment provided here"</td>
                  <td>0.806</td>
                </tr>
                <tr>
                  <th scope="row">2</th>
                  <td>"It has come to the Board's attention that one of our members may or may not have been seen in the
                    Ancient Forest We are taking this matter seriously and will be contacting our other members should we
                    learn of their location In the meantime seek out and destroy the beast"</td>
                  <td>0.7200</td>
                </tr>
                <tr>
                  <th scope="row">3</th>
                  <td>"I just spotted 7 Diablos I thought they were just goofin' around What could they possibly be doin' up in the Jungle
                     I'm goin' home now Hunter please find out for me"</td>
                  <td>0.6151</td>
                </tr>
                <tr>
                  <th scope="row">4</th>
                  <td>"I've seen my share of Pink Rathian and I still can't wrap my head around how they pull this off They go from pink to purple
                    in a matter of seconds and then there's that rumbling sound and boom I drop all my ingredients on the way back to camp"</td>
                  <td>0.6131</td>
                </tr>
                <tr>
                  <th scope="row">5</th>
                  <td>"The Commander of this expedition has been spotted Subspecies like these are highly valuable to the Commission's research efforts Get going hunter"</td>
                  <td>0.4434</td>
                </tr>
              </tbody>
            </table>

            <p>Malheureusement, nous ne sommes pas parvenues à corriger le code de façon à pouvoir établir les scores GRUEN pour nos quêtes en prenant en compte
              la ponctuation ! Les scores présentés dans le tableau 1 ci-dessus ne sont donc pas représentatifs de ce qu'on aurait pu avoir ! Les scores obtenus
              dépendent complètement de lisibilité des quêtes dépourvues de leur ponctuation. Idéalement, nous voulions une mesure automatique indiquant la qualité
            du langage de nos quêtes générées.</p>
							   </ul>					</section><!--//section-->
               </section>
			    </article><!--//docs-article-->


		        <article class="docs-article" id="section-5">
				    <header class="docs-header">
					    <h1 class="docs-heading">Essai avec BERT</h1>
					    	<ul style="text-align: justify;">
              <li>Le modèle utilisé est <b>BERT</b> — nous cherchions à comparer les performances de ce dernier à <b>GPT-2</b>.</li>
							<li>L'entraînement du modèle a été réalisé à l'aide du script <code>bert.py</code>. Nous avons passé au modèle le même corpus d'entraînement (format <code>.txt</code>) qu'au modèle GPT-2.
              L'entraînement s'est déroulé sur <b>3 epoch</b> avec des batches de taille 4.</li>
							<li>À l'instar du modèle GPT-2, le prompt de quête est construit <b>en fonction des entrées de l'utilisateur</b> :  <code>Map : {map} | Dificulty : {level} | Target : {monstres}</code>.
              Suite à ce prompt, le modèle génère l'<b>objectif</b> et la <b>description</b> de la quête correspondante. La quête générée est finalement
            décodée à l'aide du <em>tokenizer</em>.</li>
              <li>Les résultats obtenus étaient insatisfaisants : le modèle finetuné ne parvenait tout simplement <i>pas à générer des tokens corrects</i>.
                Il était donc <b>impossible</b> d'obtenir, pour un prompt donné, un objectif et une description <i>lisibles</i>.
                Cela est notamment lié à l'architecture de BERT, qui ne dispose pas d'un décodeur, mais <i>seulement</i> d'un <b>encodeur</b> (<i>encoder-only</i>) [Wang & Cho, 2019], ce qui rend difficile leur utilisation pour générer du texte, malgré leur <b>nature bidirectionnelle</b>.
                Cela fait de BERT une exception parmis les LLMs. [Patel et. al, 2023]</li>
                <li><i>Attention</i>, son utilisation pour la génération de texte n'est <i>pas impossible</i> !
                Mais, elle demande plus de travail et de recherche pour guider le modèle comme nous le souhaitons. Dans tous les cas,
                GPT-2 semblait plus adapté pour notre projet.</li>

						</ul>

			    <article class="docs-article" id="section-6">
				    <header class="docs-header">
					    <h1 class="docs-heading">Contact</h1>
					    <section class="docs-intro">
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-7-1">
						<div style="text-align: center; display: flex; justify-content: center;">

							<div style="margin: 10px;">
								<img src="image/Icon2.jpg" alt="Image 2" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover;">
								<div style="background-color: rgba(0, 0, 0, 0.5); padding: 10px; color: white; text-align: center;">
									<p>Natacha Miniconi</p>
								</div>
							</div>
							<div style="margin: 10px;">
								<img src="image/Icon3.jpg" alt="Image 3" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover;">
								<div style="background-color: rgba(0, 0, 0, 0.5); padding: 10px; color: white; text-align: center;">
									<p>Léna Gaubert</p>
								</div>
							</div>
							<div style="margin: 10px;">
								<img src="image/img4.jpg" alt="Image 3" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover;">
								<div style="background-color: rgba(0, 0, 0, 0.5); padding: 10px; color: white; text-align: center;">
									<p>Qinliang QI</p>
								</div>
							</div>
							<div style="margin: 10px;">
								<img src="image/Icon1.jpg" alt="Image 1" style="width: 150px; height: 150px; border-radius: 50%; object-fit: cover;">
								<div style="background-color: rgba(0, 0, 0, 0.5); padding: 10px; color: white; text-align: center;">
									<p>Mascotte Perl, Pro en NLP</p>
								</div>
							</div>					</section><!--//section-->

			    </article><!--//docs-article-->


			    <article class="docs-article" id="section-7">
				    <header class="docs-header">
					    <h1 class="docs-heading">Bibliographie</h1>
					    <section class="docs-intro">
              			</section><!--//docs-intro-->
              <ol>
                <li>Capcom. <cite>Monster Hunter</cite> franchise [video game], 2004.</li>
                <li>Capcom. <cite>Teppen</cite> [video game], 2019.</li>
               <li>Wangzhen Zhu, Suma Bhat.
            <cite>GRUEN for Evaluating Linguistic Quality of Generated Text</cite>, 2020.</li>
             <li>Celikyilmaz et. al.
          <cite>Evaluation of Text Generation: A Survey.</cite>, 2021.</li>
             <li>van der Lee et. al.
          <cite>Human evaluation of automatically generated text: Current trends and best practice guidelines</cite>, 2020.</li>
              <li>Alex Wang et Kyunghyun Cho. <cite> Bert has a mouth, and it must speak: Bert as a markov random
field language model.</cite> Dans <cite>Proceedings of the Workshop on Methods for Optimizing and Evaluating
Neural Language Generation</cite>, pp. 30-36, 2019.</li>
              <li>Patel et. al. <cite>Bidirectional Language Models are also few-shot learners</cite>, 2023.</li>
        </ol>
            </header>

			    </article><!--//docs-article-->

			    </article><!--//docs-article-->

			    <footer class="footer">
				    <div class="container text-center py-5">
				        <!--/* This template is free as long as you keep the footer attribution link. If you'd like to use the template without the attribution link, you can buy the commercial license via our website: themes.3rdwavemedia.com Thank you for your support. :) */-->
			            <small class="copyright">Designed with <span class="sr-only">love</span><i class="fas fa-heart" style="color: #fb866a;"></i> by <a class="theme-link" href="http://themes.3rdwavemedia.com" target="_blank">Xiaoying Riley</a> for developers</small>
				        <ul class="social-list list-unstyled pt-4 mb-0">
						    <li class="list-inline-item"><a href="#"><i class="fab fa-github fa-fw"></i></a></li>

				        </ul><!--//social-list-->
				    </div>
			    </footer>
		    </div>
	    </div>
    </div><!--//docs-wrapper-->


    <!-- Javascript -->
    <script src="assets/plugins/popper.min.js"></script>
    <script src="assets/plugins/bootstrap/js/bootstrap.min.js"></script>


    <!-- Page Specific JS -->
    <script src="assets/plugins/smoothscroll.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/highlight.min.js"></script>
    <script src="assets/js/highlight-custom.js"></script>
    <script src="assets/plugins/simplelightbox/simple-lightbox.min.js"></script>
    <script src="assets/plugins/gumshoe/gumshoe.polyfills.min.js"></script>
    <script src="assets/js/docs.js"></script>

</body>
</html>
